{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ginic(actual, pred):\n",
    "    actual = np.asarray(actual) #In case, someone passes Series or list\n",
    "    n = 50\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    "\n",
    "# ## Load Data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "#test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "d_median = train.median(axis=0)\n",
    "d_mean = train.mean(axis=0)\n",
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c:\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(int)\n",
    "    return df\n",
    "\n",
    "def prepare_data(dataset,Useless_fatures, name_target = 'target', replace_Nan = 0):\n",
    "    if replace_Nan != False:\n",
    "        dataset.replace(-1, replace_Nan )\n",
    "\n",
    "    if name_target in dataset:\n",
    "        X = dataset.drop([name_target],1)\n",
    "        X = X.drop(Useless_fatures,1)\n",
    "        \n",
    "        label = pd.DataFrame()\n",
    "        label['real'] = dataset[name_target]\n",
    "        label['inv']  = 1-dataset[name_target]\n",
    "        y = label.as_matrix()\n",
    "        print(len(X), \" \",len(label))\n",
    "        return X , y\n",
    "    else:\n",
    "        return dataset.drop(Useless_fatures,1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sub(test,model,sess,best_validation_accuracy = ''):\n",
    "    print('creating submission results...')\n",
    "    saver.restore(session,'models/my_model')\n",
    "    #X_sub = transform_df(test)\n",
    "    X_sub = prepare_data(test,['id'])\n",
    "    sub1 = sess.run(model,feed_dict={x:X_sub[:int(892816/2)]})\n",
    "    sub2 = sess.run(model,feed_dict={x:X_sub[int(892816/2):]})\n",
    "    sub_tot = np.append(sub1,sub2,axis=0)\n",
    "    max_index= sub_tot.argmax(1)\n",
    "    max_res = np.amax(sub_tot,1)\n",
    "    sub_tot = [n if max_index[counter] == 1 else 1-n for counter,n in zip(range(len(max_res)),max_res)]\n",
    "    print(len(sub_tot))\n",
    "    test_id = test.id.values    \n",
    "    print(len(test_id))\n",
    "    sub = pd.DataFrame()\n",
    "    sub['id'] = test_id\n",
    "    sub['target'] = sub_tot\n",
    "    sub.to_csv('submission_{}.csv'.format(int(best_validation_accuracy*100)) ,index = False)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "def print_accuracy( r_valu = False, feed = feed_dict_test):\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed)\n",
    "    if r_valu:\n",
    "        return acc\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130164   130164\n",
      "3000   3000\n",
      "Size of:\n",
      "- Training-set:\t\t127560\n",
      "- Test-set:\t\t2604\n"
     ]
    }
   ],
   "source": [
    "target_0 = train['target']==0\n",
    "target_1 = train['target']==1\n",
    "train_down_samp = train[target_1].append(train[target_0][0:int(sum(target_1)*3)])\n",
    "train_down_samp = train_down_samp.append(train[target_1])\n",
    "train_down_samp = train_down_samp.append(train[target_1])\n",
    "#sum(new_train['target']==1)/len(new_train)\n",
    "\n",
    "Useless_fatures = ['ps_ind_10_bin','ps_ind_11_bin','ps_ind_13_bin','ps_calc_20_bin','ps_ind_12_bin','ps_calc_15_bin','id']\n",
    "#Useless_fatures = train.columns[train.columns.str.startswith('ps_calc_')] \n",
    "#new_df_train = transform_df(train_down_samp)\n",
    "X_50 , y_50 = prepare_data(train_down_samp,['id'])\n",
    "X_real_test, y_real_test = prepare_data(train[0:3000],['id'])\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_50 , y_50, test_size=0.02)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(X_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 51.0%\n"
     ]
    }
   ],
   "source": [
    "#input size\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = 2 #y_test.shape[1]\n",
    "\n",
    "\n",
    "# ### Placeholder variables\n",
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_true = tf.placeholder(tf.float32, [None,num_classes]) ##[None, num_classes]\n",
    "y_true_cls = tf.argmax(y_true, axis=1)\n",
    "\n",
    "# ### Helper-functions for creating new variables\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
    "\n",
    "\n",
    "# ### Helper-function for creating a new Fully-Connected Layer\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,  # Use Rectified Linear Unit (ReLU)?\n",
    "                 use_sig=False):\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif use_sig:\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# ###  Layers\n",
    "#hyperparameters\n",
    "n_nodes_hl1 = 200\n",
    "n_nodes_hl2 = 600\n",
    "n_nodes_hl3 = 100\n",
    "\n",
    "layer_h1 = new_fc_layer(input=x,\n",
    "                         num_inputs=input_size,\n",
    "                         num_outputs=n_nodes_hl1,\n",
    "                         use_relu=True)\n",
    "\n",
    "layer_h2 = new_fc_layer(input=layer_h1,\n",
    "                         num_inputs=n_nodes_hl1,\n",
    "                         num_outputs=n_nodes_hl2,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "\n",
    "layer_h3 = new_fc_layer(input=layer_h2,\n",
    "                         num_inputs=n_nodes_hl2,\n",
    "                         num_outputs=n_nodes_hl3,\n",
    "                         use_relu=False,\n",
    "                         use_sig = True)\n",
    "\n",
    "y_pred = new_fc_layer(input=layer_h3,\n",
    "                         num_inputs=n_nodes_hl3,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False,\n",
    "                         use_sig = False)\n",
    "y_pred_soft_max = tf.nn.softmax(y_pred)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "# ### Cost-function and Optimizer\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#global_step = tf.Variable(0, trainable=False)\n",
    "#starter_learning_rate = 0.1\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           #100000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "##########################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,\n",
    "                                                        labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "#error = tf.square(y_pred - y_true)/2 \n",
    "#cost = tf.reduce_sum(error + error* y_true*5)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(cost)  #  1e-4\n",
    "##########################\n",
    "\n",
    "# ### Performance measures\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# ### Helper-functions to show performance\n",
    "feed_dict_test = {x: X_test, y_true: y_test}  #.values.reshape([-1,1])\n",
    "\n",
    "def print_accuracy( r_valu = False, feed = feed_dict_test):\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed)\n",
    "    if r_valu:\n",
    "        return acc\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "\n",
    "print_accuracy()\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  #search tf.saver.tensorflow\n",
    "\n",
    "# ### Helper-function to perform optimization iterations\n",
    "batch_size = 50\n",
    "best_validation_accuracy = 0\n",
    "def optimize( hm_epochs = 10, learning_r = 1e-4):\n",
    "    global best_validation_accuracy\n",
    "    global global_step\n",
    "    for epoch in range(hm_epochs):\n",
    "    \t\tepoch_loss = 0\n",
    "    \t\ti = 0\n",
    "    \t\tlen_train = X_train.shape[0]\n",
    "    \t\twhile i < len_train:\n",
    "    \t\t\tstart = i\n",
    "    \t\t\tend = i + batch_size\n",
    "                #global_step++\n",
    "                \n",
    "    \t\t\tbatch_x = np.array(X_train[start:end])\n",
    "    \t\t\tbatch_y = np.array(y_train[start:end])    #.values.reshape([-1,1]))\n",
    "    \t\t\t# Put the batch into a dict with the proper names\n",
    "    \t\t\t# for placeholder variables in the TensorFlow graph.\n",
    "    \t\t\t# Note that the placeholder for y_true_cls is not set\n",
    "    \t\t\t# because it is not used during training.\n",
    "\n",
    "    \t\t\tfeed_dict_train = {x: batch_x,\n",
    "    \t\t\t\t\t\t\t   y_true: batch_y,\n",
    "    \t\t\t\t\t\t\t   learning_rate: learning_r}\n",
    "\n",
    "    \t\t\t# Run the optimizer using this batch of training data.\n",
    "         \t\t# TensorFlow assigns the variables in feed_dict_train\n",
    "    \t\t\t# to the placeholder variables and then runs the optimizer.\n",
    "    \t\t\t_, c = session.run([optimizer,cost], feed_dict=feed_dict_train)\n",
    "    \t\t\t#session.run(optimizer, feed_dict=feed_dict_train)\n",
    "    \t\t\tepoch_loss += c\n",
    "\n",
    "    \t\t\ti += batch_size\n",
    "\n",
    "    \t\t\tif i % 20000 == 0  or i == (len_train - 1):\n",
    "\n",
    "\n",
    "    \t\t\t\tacc_ = print_accuracy(True)\n",
    "    \t\t\t\tif acc_ > best_validation_accuracy:\n",
    "                    # Update the best-known validation accuracy.\n",
    "    \t\t\t\t\tbest_validation_accuracy = acc_\n",
    "    \t\t\t\t\tsaver.save(session, 'models/my_model')\n",
    "    \t\t\t\t\timproved_str = '*'\n",
    "    \t\t\t\telse:\n",
    "    \t\t\t\t\timproved_str = ''\n",
    "\n",
    "    \t\t\t\tprint('loss: {}, data used: {} / {},  acc: {}{}'.format(epoch_loss/20,i,len_train,acc_,improved_str))\n",
    "    \t\t\t\tepoch_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "        print('Epoch {} completed out of {}'.format(epoch+1,hm_epochs))\n",
    "\n",
    "    print('Optimization ended '+ 10*'_' + 'best accuracy: {}'.format(best_validation_accuracy))\n",
    "\n",
    "\n",
    "# ## optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76958525"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.693217629007995, data used: 20000 / 127560,  acc: 0.8076037168502808\n",
      "loss: 2.755544687435031, data used: 40000 / 127560,  acc: 0.7991551756858826\n",
      "loss: 2.732390321046114, data used: 60000 / 127560,  acc: 0.7972350716590881\n",
      "loss: 2.699224737100303, data used: 80000 / 127560,  acc: 0.8018433451652527\n",
      "loss: 2.7992658603936436, data used: 100000 / 127560,  acc: 0.7876343727111816\n",
      "loss: 3.0008137419819834, data used: 120000 / 127560,  acc: 0.7918586730957031\n",
      "loss: 2.8270096432417633, data used: 20000 / 127560,  acc: 0.8072196245193481\n",
      "loss: 2.626630991511047, data used: 40000 / 127560,  acc: 0.79877108335495\n",
      "loss: 2.65242955237627, data used: 60000 / 127560,  acc: 0.7991551160812378\n",
      "loss: 2.6518537111580374, data used: 80000 / 127560,  acc: 0.8072196841239929\n",
      "loss: 2.6340655889362097, data used: 100000 / 127560,  acc: 0.7845621705055237\n",
      "loss: 2.7737218506634234, data used: 120000 / 127560,  acc: 0.7983871102333069\n",
      "loss: 2.7876395028084517, data used: 20000 / 127560,  acc: 0.8060675859451294\n",
      "loss: 2.6878480702638625, data used: 40000 / 127560,  acc: 0.7972350120544434\n",
      "loss: 2.6731835259124637, data used: 60000 / 127560,  acc: 0.804531455039978\n",
      "loss: 2.7072469841688873, data used: 80000 / 127560,  acc: 0.8083717226982117*\n",
      "loss: 2.758935556560755, data used: 100000 / 127560,  acc: 0.782642126083374\n",
      "loss: 2.8042977077886464, data used: 120000 / 127560,  acc: 0.792242705821991\n",
      "loss: 2.6698660966008902, data used: 20000 / 127560,  acc: 0.8110598921775818*\n",
      "loss: 2.5098921690136193, data used: 40000 / 127560,  acc: 0.7995391488075256\n",
      "loss: 2.615713675133884, data used: 60000 / 127560,  acc: 0.8010752201080322\n",
      "loss: 2.610872922092676, data used: 80000 / 127560,  acc: 0.8072196245193481\n",
      "loss: 2.5880313573405145, data used: 100000 / 127560,  acc: 0.7968509197235107\n",
      "loss: 2.742082901299, data used: 120000 / 127560,  acc: 0.798003077507019\n",
      "loss: 2.9559826439246537, data used: 20000 / 127560,  acc: 0.8083717226982117\n",
      "loss: 2.601899950020015, data used: 40000 / 127560,  acc: 0.7914745807647705\n",
      "loss: 2.638825033418834, data used: 60000 / 127560,  acc: 0.8045315742492676\n",
      "loss: 2.607090762257576, data used: 80000 / 127560,  acc: 0.8114439249038696*\n",
      "loss: 2.5384509161114694, data used: 100000 / 127560,  acc: 0.7930107116699219\n",
      "loss: 2.601622617803514, data used: 120000 / 127560,  acc: 0.8052995204925537\n",
      "loss: 2.8276865327730776, data used: 20000 / 127560,  acc: 0.807603657245636\n",
      "loss: 2.5645142519846558, data used: 40000 / 127560,  acc: 0.8064516186714172\n",
      "loss: 2.607798002194613, data used: 60000 / 127560,  acc: 0.8037634491920471\n",
      "loss: 2.557752199470997, data used: 80000 / 127560,  acc: 0.8026113510131836\n",
      "loss: 2.5240744618698954, data used: 100000 / 127560,  acc: 0.79877108335495\n",
      "loss: 2.5930496806278827, data used: 120000 / 127560,  acc: 0.8033794164657593\n",
      "loss: 2.611287582851946, data used: 20000 / 127560,  acc: 0.8006911873817444\n",
      "loss: 2.6491620875895023, data used: 40000 / 127560,  acc: 0.7907065749168396\n",
      "loss: 2.6586076905950904, data used: 60000 / 127560,  acc: 0.7903225421905518\n",
      "loss: 2.74519109968096, data used: 80000 / 127560,  acc: 0.8049154281616211\n",
      "loss: 2.577388375811279, data used: 100000 / 127560,  acc: 0.7941628098487854\n",
      "loss: 2.619828298687935, data used: 120000 / 127560,  acc: 0.8072197437286377\n",
      "loss: 2.4712505588307976, data used: 20000 / 127560,  acc: 0.8064516186714172\n",
      "loss: 2.4490839833393694, data used: 40000 / 127560,  acc: 0.7953149080276489\n",
      "loss: 2.688444177992642, data used: 60000 / 127560,  acc: 0.8049154877662659\n",
      "loss: 2.5612535897642372, data used: 80000 / 127560,  acc: 0.8079876899719238\n",
      "loss: 2.532335170917213, data used: 100000 / 127560,  acc: 0.7953149080276489\n",
      "loss: 2.5722342915832996, data used: 120000 / 127560,  acc: 0.8064515590667725\n",
      "loss: 2.5047227246686816, data used: 20000 / 127560,  acc: 0.8087557554244995\n",
      "loss: 2.5293075000867247, data used: 40000 / 127560,  acc: 0.7941628694534302\n",
      "loss: 2.841198127903044, data used: 60000 / 127560,  acc: 0.8018433451652527\n",
      "loss: 2.4781159656122327, data used: 80000 / 127560,  acc: 0.8206604719161987*\n",
      "loss: 2.4908710500225424, data used: 100000 / 127560,  acc: 0.7960829734802246\n",
      "loss: 2.672350916452706, data used: 120000 / 127560,  acc: 0.8029954433441162\n",
      "loss: 2.3861361226066946, data used: 20000 / 127560,  acc: 0.8156681656837463\n",
      "loss: 2.3171258298680186, data used: 40000 / 127560,  acc: 0.8049155473709106\n",
      "loss: 2.4260605223476888, data used: 60000 / 127560,  acc: 0.8102918267250061\n",
      "loss: 2.391742610000074, data used: 80000 / 127560,  acc: 0.812980055809021\n",
      "loss: 2.445945500023663, data used: 100000 / 127560,  acc: 0.7968509793281555\n",
      "loss: 2.5012599866837264, data used: 120000 / 127560,  acc: 0.8099077939987183\n",
      "loss: 2.9957353113219143, data used: 20000 / 127560,  acc: 0.8041474223136902\n",
      "loss: 2.4192607378587128, data used: 40000 / 127560,  acc: 0.8083717226982117\n",
      "loss: 2.44725581780076, data used: 60000 / 127560,  acc: 0.8064516186714172\n",
      "loss: 2.540838195383549, data used: 80000 / 127560,  acc: 0.8114438652992249\n",
      "loss: 2.4716320425271987, data used: 100000 / 127560,  acc: 0.8049154877662659\n",
      "loss: 2.428076313249767, data used: 120000 / 127560,  acc: 0.8064516186714172\n",
      "loss: 2.3359045354649424, data used: 20000 / 127560,  acc: 0.8145160675048828\n",
      "loss: 2.2564724907279015, data used: 40000 / 127560,  acc: 0.79877108335495\n",
      "loss: 2.3544515052810313, data used: 60000 / 127560,  acc: 0.8064516186714172\n",
      "loss: 2.3890383880585433, data used: 80000 / 127560,  acc: 0.8052995204925537\n",
      "loss: 2.767708718962967, data used: 100000 / 127560,  acc: 0.7937787771224976\n",
      "loss: 2.6484209798276424, data used: 120000 / 127560,  acc: 0.8022273182868958\n",
      "loss: 2.3709675796329974, data used: 20000 / 127560,  acc: 0.8099077939987183\n",
      "loss: 2.3286770937032997, data used: 40000 / 127560,  acc: 0.8079876899719238\n",
      "loss: 2.297143970802426, data used: 60000 / 127560,  acc: 0.8079877495765686\n",
      "loss: 2.321408126503229, data used: 80000 / 127560,  acc: 0.8221966028213501*\n",
      "loss: 2.436695470008999, data used: 100000 / 127560,  acc: 0.8033794164657593\n",
      "loss: 2.4054590756073595, data used: 120000 / 127560,  acc: 0.816052258014679\n",
      "loss: 2.5091700199991465, data used: 20000 / 127560,  acc: 0.8175882697105408\n",
      "loss: 2.346040382422507, data used: 40000 / 127560,  acc: 0.809907853603363\n",
      "loss: 2.4488389192149045, data used: 60000 / 127560,  acc: 0.8026113510131836\n",
      "loss: 2.430928399041295, data used: 80000 / 127560,  acc: 0.8083717226982117\n",
      "loss: 2.5120551959611475, data used: 100000 / 127560,  acc: 0.794546902179718\n",
      "loss: 2.52197330519557, data used: 120000 / 127560,  acc: 0.816052258014679\n",
      "loss: 2.3849229622632264, data used: 20000 / 127560,  acc: 0.8152841925621033\n",
      "loss: 2.326440079510212, data used: 40000 / 127560,  acc: 0.8087556958198547\n",
      "loss: 2.271886624023318, data used: 60000 / 127560,  acc: 0.8233486413955688*\n",
      "loss: 2.267913333885372, data used: 80000 / 127560,  acc: 0.826036810874939*\n",
      "loss: 2.253800143674016, data used: 100000 / 127560,  acc: 0.7999231815338135\n",
      "loss: 2.31039502248168, data used: 120000 / 127560,  acc: 0.816436231136322\n",
      "loss: 2.237505107372999, data used: 20000 / 127560,  acc: 0.8110598921775818\n",
      "loss: 2.5093224942684174, data used: 40000 / 127560,  acc: 0.7991551756858826\n",
      "loss: 2.4865864308550956, data used: 60000 / 127560,  acc: 0.8095237612724304\n",
      "loss: 2.349024001322687, data used: 80000 / 127560,  acc: 0.8233487606048584\n",
      "loss: 2.2521182084456086, data used: 100000 / 127560,  acc: 0.8056836128234863\n",
      "loss: 2.347304635308683, data used: 120000 / 127560,  acc: 0.8102918863296509\n",
      "loss: 2.205812105536461, data used: 20000 / 127560,  acc: 0.8145161271095276\n",
      "loss: 2.3820980878546836, data used: 40000 / 127560,  acc: 0.8099077939987183\n",
      "loss: 2.3526855764910577, data used: 60000 / 127560,  acc: 0.8202764987945557\n",
      "loss: 2.2694692300632595, data used: 80000 / 127560,  acc: 0.81950843334198\n",
      "loss: 2.3077598644420503, data used: 100000 / 127560,  acc: 0.8003071546554565\n",
      "loss: 2.3181125190109015, data used: 120000 / 127560,  acc: 0.8149001598358154\n",
      "loss: 2.5023763298988344, data used: 20000 / 127560,  acc: 0.8137480616569519\n",
      "loss: 2.1413833322003484, data used: 40000 / 127560,  acc: 0.8156682252883911\n",
      "loss: 2.167942323349416, data used: 60000 / 127560,  acc: 0.8191244006156921\n",
      "loss: 2.228667022101581, data used: 80000 / 127560,  acc: 0.8083717226982117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.406407150719315, data used: 100000 / 127560,  acc: 0.7999231815338135\n",
      "loss: 2.405604849755764, data used: 120000 / 127560,  acc: 0.8145160675048828\n",
      "loss: 2.1946611884981393, data used: 20000 / 127560,  acc: 0.8149001598358154\n",
      "loss: 2.0495502300560475, data used: 40000 / 127560,  acc: 0.8152841925621033\n",
      "loss: 2.0950528444722294, data used: 60000 / 127560,  acc: 0.8160521984100342\n",
      "loss: 2.132184525951743, data used: 80000 / 127560,  acc: 0.8179723620414734\n",
      "loss: 2.351427699625492, data used: 100000 / 127560,  acc: 0.8018433451652527\n",
      "loss: 2.4363565424457194, data used: 120000 / 127560,  acc: 0.8114439249038696\n",
      "loss: 2.386872349306941, data used: 20000 / 127560,  acc: 0.8091397881507874\n",
      "loss: 2.142755561042577, data used: 40000 / 127560,  acc: 0.8122119307518005\n",
      "loss: 2.1847850672900675, data used: 60000 / 127560,  acc: 0.8087557554244995\n",
      "loss: 2.25949579346925, data used: 80000 / 127560,  acc: 0.8125959634780884\n",
      "loss: 2.217010240443051, data used: 100000 / 127560,  acc: 0.8079877495765686\n",
      "loss: 2.1799156052991746, data used: 120000 / 127560,  acc: 0.8168203234672546\n",
      "loss: 2.1299576653167605, data used: 20000 / 127560,  acc: 0.8237326741218567\n",
      "loss: 2.010779394581914, data used: 40000 / 127560,  acc: 0.8118279576301575\n",
      "loss: 2.1306934780441225, data used: 60000 / 127560,  acc: 0.8137480616569519\n",
      "loss: 2.29318527225405, data used: 80000 / 127560,  acc: 0.8091398477554321\n",
      "loss: 2.414406857546419, data used: 100000 / 127560,  acc: 0.7983870506286621\n",
      "loss: 2.343799461238086, data used: 120000 / 127560,  acc: 0.816052258014679\n",
      "loss: 2.0564594676718118, data used: 20000 / 127560,  acc: 0.8214285373687744\n",
      "loss: 1.95351208653301, data used: 40000 / 127560,  acc: 0.8122119307518005\n",
      "loss: 2.090897349268198, data used: 60000 / 127560,  acc: 0.8175883293151855\n",
      "loss: 2.2629668422043325, data used: 80000 / 127560,  acc: 0.8122119903564453\n",
      "loss: 2.1783442477695645, data used: 100000 / 127560,  acc: 0.8033794164657593\n",
      "loss: 2.174119897186756, data used: 120000 / 127560,  acc: 0.8260368704795837*\n",
      "loss: 2.160945223644376, data used: 20000 / 127560,  acc: 0.8152841329574585\n",
      "loss: 2.0198146416805685, data used: 40000 / 127560,  acc: 0.8087557554244995\n",
      "loss: 2.2495755924843253, data used: 60000 / 127560,  acc: 0.812980055809021\n",
      "loss: 2.098812071792781, data used: 80000 / 127560,  acc: 0.8102918863296509\n",
      "loss: 2.0902076452970504, data used: 100000 / 127560,  acc: 0.8133640885353088\n",
      "loss: 2.2657992033287884, data used: 120000 / 127560,  acc: 0.8145161271095276\n",
      "loss: 2.127298767119646, data used: 20000 / 127560,  acc: 0.8202764987945557\n",
      "loss: 1.9928994744084776, data used: 40000 / 127560,  acc: 0.8152841925621033\n",
      "loss: 1.9644625327549874, data used: 60000 / 127560,  acc: 0.8248847723007202\n",
      "loss: 2.0031359465792775, data used: 80000 / 127560,  acc: 0.8229647278785706\n",
      "loss: 2.1954300383105876, data used: 100000 / 127560,  acc: 0.8114439249038696\n",
      "loss: 2.354653148725629, data used: 120000 / 127560,  acc: 0.815284252166748\n",
      "loss: 2.0603659176267684, data used: 20000 / 127560,  acc: 0.8241167068481445\n",
      "loss: 1.8897304299287498, data used: 40000 / 127560,  acc: 0.8214284777641296\n",
      "loss: 1.9315677829086781, data used: 60000 / 127560,  acc: 0.8229646682739258\n",
      "loss: 1.9528487829491497, data used: 80000 / 127560,  acc: 0.8260369300842285*\n",
      "loss: 2.128454996738583, data used: 100000 / 127560,  acc: 0.8095238208770752\n",
      "loss: 2.3477159118279816, data used: 120000 / 127560,  acc: 0.8091397881507874\n",
      "loss: 2.0126832485198975, data used: 20000 / 127560,  acc: 0.8245007991790771\n",
      "loss: 2.056016098149121, data used: 40000 / 127560,  acc: 0.818356454372406\n",
      "loss: 2.1203007446601987, data used: 60000 / 127560,  acc: 0.8252688050270081\n",
      "loss: 2.0500879546627404, data used: 80000 / 127560,  acc: 0.8225806355476379\n",
      "loss: 2.0029679868370294, data used: 100000 / 127560,  acc: 0.806451678276062\n",
      "loss: 2.0422051357105375, data used: 120000 / 127560,  acc: 0.8172042965888977\n",
      "loss: 2.007259938120842, data used: 20000 / 127560,  acc: 0.8175883293151855\n",
      "loss: 1.9657713704742492, data used: 40000 / 127560,  acc: 0.8152841329574585\n",
      "loss: 1.9333682612515986, data used: 60000 / 127560,  acc: 0.8291091322898865*\n",
      "loss: 2.1365292282775044, data used: 80000 / 127560,  acc: 0.8122119903564453\n",
      "loss: 2.1137901566922666, data used: 100000 / 127560,  acc: 0.8095237612724304\n",
      "loss: 2.137738164514303, data used: 120000 / 127560,  acc: 0.8091397881507874\n",
      "loss: 1.97883878685534, data used: 20000 / 127560,  acc: 0.8172042965888977\n",
      "loss: 1.9946899745613336, data used: 40000 / 127560,  acc: 0.8152841925621033\n",
      "loss: 2.0221381770446896, data used: 60000 / 127560,  acc: 0.828341007232666\n",
      "loss: 2.1930391949601473, data used: 80000 / 127560,  acc: 0.8248847723007202\n",
      "loss: 2.0048936706967653, data used: 100000 / 127560,  acc: 0.8087556958198547\n",
      "loss: 1.9789975105784834, data used: 120000 / 127560,  acc: 0.8145161867141724\n",
      "loss: 1.9789020258933305, data used: 20000 / 127560,  acc: 0.8271889686584473\n",
      "loss: 1.8815234727226198, data used: 40000 / 127560,  acc: 0.8079877495765686\n",
      "loss: 1.971406993549317, data used: 60000 / 127560,  acc: 0.8248847723007202\n",
      "loss: 1.9758853357285262, data used: 80000 / 127560,  acc: 0.8221967220306396\n",
      "loss: 1.970323833450675, data used: 100000 / 127560,  acc: 0.8149001598358154\n",
      "loss: 1.9777460576966406, data used: 120000 / 127560,  acc: 0.8187404274940491\n",
      "loss: 1.8964739613234998, data used: 20000 / 127560,  acc: 0.8287249803543091\n",
      "loss: 1.859398168232292, data used: 40000 / 127560,  acc: 0.8206605911254883\n",
      "loss: 2.112770519591868, data used: 60000 / 127560,  acc: 0.8191244006156921\n",
      "loss: 1.9021739220246672, data used: 80000 / 127560,  acc: 0.8302611112594604*\n",
      "loss: 1.9388605569489301, data used: 100000 / 127560,  acc: 0.8141321539878845\n",
      "loss: 1.9406647376716137, data used: 120000 / 127560,  acc: 0.8145161271095276\n",
      "Epoch 30 completed out of 30\n",
      "Optimization ended __________best accuracy: 0.8302611112594604\n"
     ]
    }
   ],
   "source": [
    "optimize(30,1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating submission results...\n",
      "INFO:tensorflow:Restoring parameters from models/my_model\n",
      "892816\n",
      "892816\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "make_sub(test,y_pred_soft_max,session,best_validation_accuracy = best_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 90.4%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.99982601,\n",
       " 0.89293063,\n",
       " 0.99038321,\n",
       " 0.9428665,\n",
       " 0.81507599,\n",
       " 0.99999464,\n",
       " 0.9560079,\n",
       " 0.096562206745147705]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_real_test = {x: X_real_test, y_true:y_real_test}\n",
    "print_accuracy(feed=feed_dict_real_test)\n",
    "a = session.run(y_pred_soft_max,feed_dict_real_test)\n",
    "b = a[:10]\n",
    "c= b.argmax(1)\n",
    "np.amax(b,1)\n",
    "[n if c[m] == 1 else 1-n for m,n in zip(range(len(np.amax(b,1))),np.amax(b,1))]\n",
    "#np.round(b,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_real_test[y_true][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ],\n",
       "       [ 0.004     ,  0.99599999],\n",
       "       [ 0.40700001,  0.59299999],\n",
       "       [ 0.066     ,  0.93400002],\n",
       "       [ 0.51300001,  0.48699999],\n",
       "       [ 0.61299998,  0.38699999],\n",
       "       [ 0.001     ,  0.99900001],\n",
       "       [ 0.37099999,  0.62900001],\n",
       "       [ 0.551     ,  0.449     ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
