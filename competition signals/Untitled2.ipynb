{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(20)\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "import t\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "'''Data loading & preprocessing\n",
    "'''\n",
    "\n",
    "X_train = pd.read_csv('data/train.csv')\n",
    "X_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "X_train, y_train = X_train.iloc[:,2:], X_train.target\n",
    "X_test, test_id = X_test.iloc[:,1:], X_test.id\n",
    "\n",
    "#OHE / some feature engineering adapted from the1owl kernel at:\n",
    "#https://www.kaggle.com/the1owl/forza-baseline/code\n",
    "\n",
    "#excluded columns based on snowdog's old school nn kernel at:\n",
    "#https://www.kaggle.com/snowdog/old-school-nnet\n",
    "\n",
    "X_train['negative_one_vals'] = np.sum((X_train==-1).values, axis=1)\n",
    "X_test['negative_one_vals'] = np.sum((X_test==-1).values, axis=1)\n",
    "\n",
    "to_drop = ['ps_car_11_cat', 'ps_ind_14', 'ps_car_11', 'ps_car_14', 'ps_ind_06_bin', \n",
    "           'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', \n",
    "           'ps_ind_13_bin']\n",
    "\n",
    "cols_use = [c for c in X_train.columns if (not c.startswith('ps_calc_'))\n",
    "             & (not c in to_drop)]\n",
    "             \n",
    "X_train = X_train[cols_use]\n",
    "X_test = X_test[cols_use]\n",
    "\n",
    "one_hot = {c: list(X_train[c].unique()) for c in X_train.columns}\n",
    "\n",
    "#note that this encodes the negative_one_vals column as well\n",
    "for c in one_hot:\n",
    "    if len(one_hot[c])>2 and len(one_hot[c]) < 105:\n",
    "        for val in one_hot[c]:\n",
    "            newcol = c + '_oh_' + str(val)\n",
    "            X_train[newcol] = (X_train[c].values == val).astype(np.int)\n",
    "            X_test[newcol] = (X_test[c].values == val).astype(np.int)\n",
    "        X_train.drop(labels=[c], axis=1, inplace=True)\n",
    "        X_test.drop(labels=[c], axis=1, inplace=True)\n",
    "            \n",
    "X_train = X_train.replace(-1, np.NaN)  # Get rid of -1 while computing interaction col\n",
    "X_test = X_test.replace(-1, np.NaN)\n",
    "\n",
    "X_train['ps_car_13_x_ps_reg_03'] = X_train['ps_car_13'] * X_train['ps_reg_03']\n",
    "X_test['ps_car_13_x_ps_reg_03'] = X_test['ps_car_13'] * X_test['ps_reg_03']\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_test = X_test.fillna(-1)\n",
    "\n",
    "'''Gini scoring function\n",
    "'''\n",
    "\n",
    "#gini scoring function from kernel at: \n",
    "#https://www.kaggle.com/tezdhar/faster-gini-calculation\n",
    "def ginic(actual, pred):\n",
    "    n = len(actual)\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_c[-1] - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    " \n",
    "def gini_normalizedc(a, p):\n",
    "    return ginic(a, p) / ginic(a, a)\n",
    "\n",
    "'''5-fold neural network training \n",
    "'''\n",
    "\n",
    "K = 5 #number of folds\n",
    "runs_per_fold = 3 #bagging on each fold\n",
    "\n",
    "cv_ginis = []\n",
    "y_preds = np.zeros((np.shape(X_test)[0],K))\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = K, \n",
    "                            random_state = 100, \n",
    "                            shuffle = True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5df651a4c75e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.002\u001b[0m  \u001b[0;31m# l2 regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m layer_h1 = new_fc_layer(input=x,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name = 'weight')\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='bias')\n",
    "\n",
    "\n",
    "# ### Helper-function for creating a new Fully-Connected Layer\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,  # Use Rectified Linear Unit (ReLU)?\n",
    "                 use_sig=False):\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif use_sig:\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    return layer\n",
    "\n",
    "n_nodes_hl1 = 57\n",
    "n_nodes_hl2 = 35\n",
    "n_nodes_hl3 = 15\n",
    "\n",
    "beta = 0.002  # l2 regularization\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "layer_h1 = new_fc_layer(input=x,\n",
    "                         num_inputs=input_size,\n",
    "                         num_outputs=n_nodes_hl1,\n",
    "                         use_relu=False,\n",
    "                         use_sig = True)\n",
    "drop_h1 = tf.nn.dropout(layer_h1, 0.7 + keep_prob*0.2)\n",
    "\n",
    "y_pred = new_fc_layer(input=drop_h1,\n",
    "                         num_inputs=n_nodes_hl1,\n",
    "                         num_outputs=2,\n",
    "                         use_relu=False,\n",
    "                         use_sig = False)\n",
    "y_pred_soft_max = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "var_s = tf.trainable_variables()\n",
    "loss_l2 = tf.add_n([tf.nn.l2_loss(v) for v in var_s if 'bias' not in v.name])*0.001\n",
    "loss = tf.square(y_pred_soft_max - y_true)/2\n",
    "cost = tf.reduce_mean(loss+ loss_l2)\n",
    "loss = tf.losses.sigmoid_cross_entropy(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, (f_ind, outf_ind) in enumerate(kfold.split(X_train, y_train)):\n",
    "\n",
    "    X_train_f, X_val_f = X_train.loc[f_ind].copy(), X_train.loc[outf_ind].copy()\n",
    "    y_train_f, y_val_f = y_train[f_ind], y_train[outf_ind]\n",
    "          \n",
    "    #upsampling adapted from kernel: \n",
    "    #https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283\n",
    "    pos = (pd.Series(y_train_f == 1))\n",
    "    \n",
    "    # Add positive examples\n",
    "    X_train_f = pd.concat([X_train_f, X_train_f.loc[pos]], axis=0)\n",
    "    y_train_f = pd.concat([y_train_f, y_train_f.loc[pos]], axis=0)\n",
    "    \n",
    "    # Shuffle data\n",
    "    idx = np.arange(len(X_train_f))\n",
    "    np.random.shuffle(idx)\n",
    "    X_train_f = X_train_f.iloc[idx]\n",
    "    y_train_f = y_train_f.iloc[idx]\n",
    "    \n",
    "    #track oof bagged prediction for cv scores\n",
    "    val_preds = 0\n",
    "    \n",
    "    for j in range(runs_per_fold):\n",
    "    \n",
    "        NN=Sequential()\n",
    "        NN.add(Dense(35,activation='relu',input_dim=np.shape(X_train_f)[1]))\n",
    "        NN.add(Dropout(0.3))\n",
    "        NN.add(Dense(1,activation='sigmoid'))\n",
    "        \n",
    "        NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        set_random_seed(1000*i+j)\n",
    "            \n",
    "        NN.fit(X_train_f.values, y_train_f.values, epochs=15, batch_size=2048, verbose=0)\n",
    "         \n",
    "        val_gini = gini_normalizedc(y_val_f.values, NN.predict(X_val_f.values)[:,0])   \n",
    "        print ('\\nFold %d Run %d Results *****' % (i, j))\n",
    "        print ('Validation gini: %.5f\\n' % (val_gini))\n",
    "        \n",
    "        val_preds += NN.predict(X_val_f.values)[:,0] / runs_per_fold\n",
    "        y_preds[:,i] += NN.predict(X_test.values)[:,0] / runs_per_fold\n",
    "        \n",
    "    cv_ginis.append(val_gini)\n",
    "    print ('\\nFold %i prediction cv gini: %.5f\\n' %(i,val_gini))\n",
    "    \n",
    "print('Mean out of fold gini: %.5f' % np.mean(cv_ginis))\n",
    "y_pred_final = np.mean(y_preds, axis=1)\n",
    "\n",
    "df_sub = pd.DataFrame({'id' : test_id, \n",
    "                       'target' : y_pred_final},\n",
    "                       columns = ['id','target'])\n",
    "df_sub.to_csv('NNShallow_5fold_3runs_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
