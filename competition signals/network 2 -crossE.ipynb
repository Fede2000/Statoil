{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ginic(actual, pred):\n",
    "    actual = np.asarray(actual) #In case, someone passes Series or list\n",
    "    n = 50\n",
    "    a_s = actual[np.argsort(pred)]\n",
    "    a_c = a_s.cumsum()\n",
    "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\n",
    "    return giniSum / n\n",
    "\n",
    "# ## Load Data\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "#test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "#d_median = train.median(axis=0)\n",
    "#d_mean = train.mean(axis=0)\n",
    "\n",
    "def transform_df(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    dcol = [c for c in df.columns if c not in ['id','target']]\n",
    "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\n",
    "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\n",
    "    for c in dcol:\n",
    "        if '_bin' not in c:\n",
    "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(int)\n",
    "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(int)\n",
    "    return df\n",
    "\n",
    "def prepare_data(dataset,Useless_fatures, name_target = 'target', replace_Nan = 0):\n",
    "    dataset['ps_car_13_x_ps_reg_03'] = dataset['ps_car_13'] * dataset['ps_reg_03']\n",
    "    if replace_Nan != False:\n",
    "        dataset.replace(-1, replace_Nan )\n",
    "\n",
    "    if name_target in dataset:\n",
    "        X = dataset.drop([name_target],1)\n",
    "        X = X.drop(Useless_fatures,1)\n",
    "        \n",
    "        label = pd.DataFrame()\n",
    "        label['real'] = dataset[name_target]\n",
    "        label['inv']  = 1-dataset[name_target]\n",
    "        y = label.as_matrix()\n",
    "        print(len(X), \" \",len(label))\n",
    "        return X , y\n",
    "    else:\n",
    "        return dataset.drop(Useless_fatures,1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_sub(test,model,sess,best_validation_accuracy = ''):\n",
    "    print('creating submission results...')\n",
    "    saver.restore(session,'models/my_model')\n",
    "    #X_sub = transform_df(test)\n",
    "    X_sub = prepare_data(test,['id'])\n",
    "    sub1 = sess.run(model,feed_dict={x:X_sub[:int(892816/2)],keep_prob:1.0})\n",
    "    sub2 = sess.run(model,feed_dict={x:X_sub[int(892816/2):],keep_prob:1.0})\n",
    "    sub_tot = np.append(sub1,sub2,axis=0)\n",
    "    print('Output Created!')\n",
    "    max_index= sub_tot.argmax(1)\n",
    "    max_res = np.amax(sub_tot,1)\n",
    "    sub_tot = [n if max_index[counter] == 0 else 1-n for counter,n in zip(range(len(max_res)),max_res)]\n",
    "    test_id = test.id.values    \n",
    "    sub = pd.DataFrame()\n",
    "    sub['id'] = test_id\n",
    "    sub['target'] = sub_tot\n",
    "    sub.to_csv('submission_{}.csv'.format(int(best_validation_accuracy*100)) ,index = False)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130164   130164\n",
      "3000   3000\n",
      "Size of:\n",
      "- Training-set:\t\t127560\n",
      "- Test-set:\t\t2604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "target_0 = train['target']==0\n",
    "target_1 = train['target']==1\n",
    "train_down_samp = train[target_1].append(train[target_0][0:int(sum(target_1)*3)])\n",
    "train_down_samp = train_down_samp.append(train[target_1])\n",
    "train_down_samp = train_down_samp.append(train[target_1])\n",
    "#sum(new_train['target']==1)/len(new_train)\n",
    "\n",
    "Useless_fatures = ['ps_ind_10_bin','ps_ind_11_bin','ps_ind_13_bin','ps_calc_20_bin','ps_ind_12_bin','ps_calc_15_bin','id']\n",
    "#Useless_fatures = train.columns[train.columns.str.startswith('ps_calc_')] \n",
    "#new_df_train = transform_df(train_down_samp)\n",
    "X_50 , y_50 = prepare_data(train_down_samp,['id'])\n",
    "X_real_test, y_real_test = prepare_data(shuffle(train_down_samp)[0:3000],['id'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X_50 , y_50, test_size=0.02)\n",
    "\n",
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(X_train)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 49.6%\n"
     ]
    }
   ],
   "source": [
    "#input size\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = 2 #y_test.shape[1]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# ### Placeholder variables\n",
    "x = tf.placeholder(tf.float32, [None, input_size])\n",
    "y_true = tf.placeholder(tf.float32, [None,num_classes]) ##[None, num_classes]\n",
    "y_true_cls = tf.argmax(y_true, axis=1)\n",
    "\n",
    "# ### Helper-functions for creating new variables\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name = 'weight')\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='bias')\n",
    "\n",
    "\n",
    "# ### Helper-function for creating a new Fully-Connected Layer\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,  # Use Rectified Linear Unit (ReLU)?\n",
    "                 use_sig=False):\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif use_sig:\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# ###  Layers\n",
    "#hyperparameters\n",
    "n_nodes_hl1 = 400\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 500\n",
    "beta = 0.0001  # l2 regularization\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "layer_h1 = new_fc_layer(input=x,\n",
    "                         num_inputs=input_size,\n",
    "                         num_outputs=n_nodes_hl1,\n",
    "                         use_relu=True)\n",
    "drop_h1 = tf.nn.dropout(layer_h1, keep_prob)\n",
    "\n",
    "layer_h2 = new_fc_layer(input=drop_h1,\n",
    "                         num_inputs=n_nodes_hl1,\n",
    "                         num_outputs=n_nodes_hl2,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "\n",
    "drop_h2 = tf.nn.dropout(layer_h2, keep_prob)\n",
    "\n",
    "layer_h3 = new_fc_layer(input=drop_h2,\n",
    "                         num_inputs=n_nodes_hl2,\n",
    "                         num_outputs=n_nodes_hl3,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "drop_h3 = tf.nn.dropout(layer_h3, keep_prob)\n",
    "\n",
    "\n",
    "y_pred = new_fc_layer(input=drop_h3,\n",
    "                         num_inputs=n_nodes_hl3,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False,\n",
    "                         use_sig = False)\n",
    "y_pred_soft_max = tf.nn.softmax(y_pred)\n",
    "y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# ### Cost-function and Optimizer\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "#global_step = tf.Variable(0, trainable=False)\n",
    "#starter_learning_rate = 0.1\n",
    "#learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           #100000, 0.96, staircase=True)\n",
    "\n",
    "\n",
    "##########################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,\n",
    "                                                        labels=y_true)\n",
    "\n",
    "#mse = tf.losses.mean_squared_error(y_true,y_pred_soft_max)\n",
    "\n",
    "\n",
    "#cross_weighted =  tf.multiply(cross_entropy,y_true[:,0]*0.00005)\n",
    "var_s   = tf.trainable_variables() \n",
    "lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in var_s\n",
    "                    if 'bias' not in v.name ]) * beta\n",
    "cost = tf.reduce_mean(cross_entropy )\n",
    "#error = tf.square(y_pred - y_true)/2 \n",
    "#cost = tf.reduce_sum(error + error* y_true*5)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)  #  1e-4\n",
    "##########################\n",
    "\n",
    "# ### Performance measures\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# ### Helper-functions to show performance\n",
    "feed_dict_test = {x: X_test, y_true: y_test, keep_prob: 1.0}  #.values.reshape([-1,1])\n",
    "feed_dict_real_test = {x: X_real_test, y_true:y_real_test, keep_prob: 1.0}\n",
    "\n",
    "def print_accuracy( r_valu = False, feed = feed_dict_test):\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed)\n",
    "    if r_valu:\n",
    "        return acc\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "\n",
    "print_accuracy()\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()  #search tf.saver.tensorflow\n",
    "\n",
    "# ### Helper-function to perform optimization iterations\n",
    "batch_size = 200\n",
    "best_validation_accuracy = 0\n",
    "\n",
    "# ## optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize( hm_epochs = 10, learning_r = 1e-4, until_acc = False):\n",
    "    global best_validation_accuracy\n",
    "    global global_step\n",
    "    epoch = 0\n",
    "    while epoch < hm_epochs or best_validation_accuracy<=until_acc:\n",
    "    \t\tepoch_loss = 0\n",
    "    \t\ti = 0\n",
    "    \t\tlen_train = X_train.shape[0]\n",
    "    \t\twhile i < len_train:\n",
    "    \t\t\tstart = i\n",
    "    \t\t\tend = i + batch_size\n",
    "                #global_step++\n",
    "                \n",
    "    \t\t\tbatch_x = np.array(X_train[start:end])\n",
    "    \t\t\tbatch_y = np.array(y_train[start:end])    #.values.reshape([-1,1]))\n",
    "    \t\t\t# Put the batch into a dict with the proper names\n",
    "    \t\t\t# for placeholder variables in the TensorFlow graph.\n",
    "    \t\t\t# Note that the placeholder for y_true_cls is not set\n",
    "    \t\t\t# because it is not used during training.\n",
    "\n",
    "    \t\t\tfeed_dict_train = {x: batch_x,\n",
    "    \t\t\t\t\t\t\t   y_true: batch_y,\n",
    "    \t\t\t\t\t\t\t   keep_prob: 0.8}\n",
    "\n",
    "    \t\t\t# Run the optimizer using this batch of training data.\n",
    "         \t\t# TensorFlow assigns the variables in feed_dict_train\n",
    "    \t\t\t# to the placeholder variables and then runs the optimizer.\n",
    "    \t\t\t_, c = session.run([optimizer,cost], feed_dict=feed_dict_train)\n",
    "    \t\t\t#session.run(optimizer, feed_dict=feed_dict_train)\n",
    "    \t\t\tepoch_loss += c\n",
    "\n",
    "    \t\t\ti += batch_size\n",
    "\n",
    "    \t\t\tif i % 100000 == 0  or i == (len_train - 1):\n",
    "\n",
    "\n",
    "    \t\t\t\tacc_ = print_accuracy(True,feed_dict_real_test)\n",
    "    \t\t\t\tif acc_ > best_validation_accuracy:\n",
    "                    # Update the best-known validation accuracy.\n",
    "    \t\t\t\t\t#acc_real = print_accuracy(True,feed_dict_real_test)\n",
    "    \t\t\t\t\tsaver.save(session, 'models/my_model')\n",
    "    \t\t\t\t\timproved_str = '*'\n",
    "    \t\t\t\t\tbest_validation_accuracy = acc_\n",
    "    \t\t\t\telse:\n",
    "    \t\t\t\t\timproved_str = ''\n",
    "\n",
    "    \t\t\t\tprint('loss: {}, data used: {} / {},  acc: {}{}'.format(epoch_loss/20,i,len_train,acc_,improved_str))\n",
    "    \t\t\t\tepoch_loss = 0\n",
    "    \t\tepoch +=1\n",
    "\n",
    "\n",
    "    print('Epoch {} completed out of {}'.format(epoch+1,hm_epochs))\n",
    "\n",
    "    print('Optimization ended '+ 10*'_' + 'best accuracy: {}'.format(best_validation_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'weight:0' shape=(58, 400) dtype=float32_ref>,\n",
       " <tf.Variable 'bias:0' shape=(400,) dtype=float32_ref>,\n",
       " <tf.Variable 'weight_1:0' shape=(400, 1500) dtype=float32_ref>,\n",
       " <tf.Variable 'bias_1:0' shape=(1500,) dtype=float32_ref>,\n",
       " <tf.Variable 'weight_2:0' shape=(1500, 500) dtype=float32_ref>,\n",
       " <tf.Variable 'bias_2:0' shape=(500,) dtype=float32_ref>,\n",
       " <tf.Variable 'weight_3:0' shape=(500, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'bias_3:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_validation_accuracy = 0.7\n",
    "var_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 13.680325995385648, data used: 100000 / 127560,  acc: 0.7956667542457581\n",
      "loss: 13.730568033456802, data used: 100000 / 127560,  acc: 0.7940000891685486\n",
      "loss: 13.739822039008141, data used: 100000 / 127560,  acc: 0.799000084400177\n",
      "loss: 13.748048855364322, data used: 100000 / 127560,  acc: 0.786333441734314\n",
      "loss: 13.749595218896866, data used: 100000 / 127560,  acc: 0.7970001101493835\n",
      "loss: 13.772712957859039, data used: 100000 / 127560,  acc: 0.800000011920929\n",
      "loss: 13.730621539056301, data used: 100000 / 127560,  acc: 0.7903333902359009\n",
      "loss: 13.739194665849208, data used: 100000 / 127560,  acc: 0.7986667156219482\n",
      "loss: 13.744633236527443, data used: 100000 / 127560,  acc: 0.7920000553131104\n",
      "loss: 13.775992654263973, data used: 100000 / 127560,  acc: 0.7980000972747803\n",
      "loss: 13.758577513694764, data used: 100000 / 127560,  acc: 0.7860000729560852\n",
      "loss: 13.768339984118938, data used: 100000 / 127560,  acc: 0.7896667122840881\n",
      "loss: 13.7796605437994, data used: 100000 / 127560,  acc: 0.7933334112167358\n",
      "loss: 13.70851040929556, data used: 100000 / 127560,  acc: 0.8003333806991577\n",
      "loss: 13.718384052813054, data used: 100000 / 127560,  acc: 0.8026667833328247\n",
      "loss: 13.705500535666943, data used: 100000 / 127560,  acc: 0.7893333435058594\n",
      "loss: 13.782798843085766, data used: 100000 / 127560,  acc: 0.7843334674835205\n",
      "loss: 13.724753615260123, data used: 100000 / 127560,  acc: 0.7946667671203613\n",
      "loss: 13.756138636171817, data used: 100000 / 127560,  acc: 0.7953333854675293\n",
      "loss: 13.752498926222325, data used: 100000 / 127560,  acc: 0.8043333888053894*\n",
      "loss: 13.680126625299454, data used: 100000 / 127560,  acc: 0.7886667251586914\n",
      "loss: 13.733844415843487, data used: 100000 / 127560,  acc: 0.8050000071525574*\n",
      "loss: 13.763651002943515, data used: 100000 / 127560,  acc: 0.7883332967758179\n",
      "loss: 13.761581744253636, data used: 100000 / 127560,  acc: 0.7923333644866943\n",
      "loss: 13.735825473070145, data used: 100000 / 127560,  acc: 0.780666708946228\n",
      "loss: 13.792262037098407, data used: 100000 / 127560,  acc: 0.7850000858306885\n",
      "loss: 13.695502935349941, data used: 100000 / 127560,  acc: 0.796999990940094\n",
      "loss: 13.711825536191464, data used: 100000 / 127560,  acc: 0.81166672706604*\n",
      "loss: 13.680574379861355, data used: 100000 / 127560,  acc: 0.7900000214576721\n",
      "loss: 13.780437813699246, data used: 100000 / 127560,  acc: 0.7896667122840881\n",
      "loss: 13.747383360564708, data used: 100000 / 127560,  acc: 0.7880000472068787\n",
      "loss: 13.703720958530903, data used: 100000 / 127560,  acc: 0.8050000667572021\n",
      "loss: 13.808201998472214, data used: 100000 / 127560,  acc: 0.802333414554596\n",
      "loss: 13.740930435061454, data used: 100000 / 127560,  acc: 0.812000036239624*\n",
      "loss: 13.802824313938618, data used: 100000 / 127560,  acc: 0.7966667413711548\n",
      "loss: 13.739226917922498, data used: 100000 / 127560,  acc: 0.8063333630561829\n",
      "loss: 13.750069698691368, data used: 100000 / 127560,  acc: 0.7910001277923584\n",
      "loss: 13.729460321366787, data used: 100000 / 127560,  acc: 0.800000011920929\n",
      "loss: 13.693873043358327, data used: 100000 / 127560,  acc: 0.8066667318344116\n",
      "loss: 13.709092402458191, data used: 100000 / 127560,  acc: 0.8040000796318054\n",
      "loss: 13.771146661043167, data used: 100000 / 127560,  acc: 0.8023333549499512\n",
      "loss: 13.759861765801906, data used: 100000 / 127560,  acc: 0.7890000343322754\n",
      "loss: 13.710194075107575, data used: 100000 / 127560,  acc: 0.7883334755897522\n",
      "loss: 13.72376607209444, data used: 100000 / 127560,  acc: 0.8000000715255737\n",
      "loss: 13.712411697208882, data used: 100000 / 127560,  acc: 0.7976667881011963\n",
      "loss: 13.818664836883546, data used: 100000 / 127560,  acc: 0.7926667332649231\n",
      "loss: 13.797347161173821, data used: 100000 / 127560,  acc: 0.7993334531784058\n",
      "loss: 13.719339773058891, data used: 100000 / 127560,  acc: 0.8019999861717224\n",
      "loss: 13.713193362951278, data used: 100000 / 127560,  acc: 0.8063334226608276\n",
      "loss: 13.706589618325234, data used: 100000 / 127560,  acc: 0.8000000715255737\n",
      "loss: 13.666334171593189, data used: 100000 / 127560,  acc: 0.7980000376701355\n",
      "loss: 13.77126108855009, data used: 100000 / 127560,  acc: 0.7906667590141296\n",
      "loss: 13.768183200061321, data used: 100000 / 127560,  acc: 0.7970000505447388\n",
      "loss: 13.727720603346825, data used: 100000 / 127560,  acc: 0.7946667075157166\n",
      "loss: 13.733590303361416, data used: 100000 / 127560,  acc: 0.8106667995452881\n",
      "loss: 13.73707710057497, data used: 100000 / 127560,  acc: 0.8070000410079956\n",
      "loss: 13.715328779816627, data used: 100000 / 127560,  acc: 0.7786667346954346\n",
      "loss: 13.74522993415594, data used: 100000 / 127560,  acc: 0.7986667156219482\n",
      "loss: 13.722272184491157, data used: 100000 / 127560,  acc: 0.7970001101493835\n",
      "loss: 13.753800792992115, data used: 100000 / 127560,  acc: 0.7953334450721741\n",
      "loss: 13.700289222598077, data used: 100000 / 127560,  acc: 0.8070001006126404\n",
      "loss: 13.773323598504067, data used: 100000 / 127560,  acc: 0.8043334484100342\n",
      "loss: 13.77206942886114, data used: 100000 / 127560,  acc: 0.7920000553131104\n",
      "loss: 13.77517119050026, data used: 100000 / 127560,  acc: 0.7936667203903198\n",
      "loss: 13.698749348521233, data used: 100000 / 127560,  acc: 0.8006668090820312\n",
      "loss: 13.738521388173103, data used: 100000 / 127560,  acc: 0.7943333983421326\n",
      "loss: 13.756622360646725, data used: 100000 / 127560,  acc: 0.796333372592926\n",
      "loss: 13.743774934113025, data used: 100000 / 127560,  acc: 0.7950000762939453\n",
      "loss: 13.617908865213394, data used: 100000 / 127560,  acc: 0.8086667060852051\n",
      "loss: 13.716253094375134, data used: 100000 / 127560,  acc: 0.8103334307670593\n",
      "loss: 13.798499175906182, data used: 100000 / 127560,  acc: 0.7886667251586914\n",
      "loss: 13.75110045671463, data used: 100000 / 127560,  acc: 0.7956667542457581\n",
      "loss: 13.706857697665692, data used: 100000 / 127560,  acc: 0.8026667833328247\n",
      "loss: 13.81225620508194, data used: 100000 / 127560,  acc: 0.7876667380332947\n",
      "loss: 13.687607325613499, data used: 100000 / 127560,  acc: 0.8070001006126404\n",
      "loss: 13.729588107764721, data used: 100000 / 127560,  acc: 0.8043334484100342\n",
      "loss: 13.733692878484726, data used: 100000 / 127560,  acc: 0.7933334112167358\n",
      "loss: 13.737864650785923, data used: 100000 / 127560,  acc: 0.7980000972747803\n",
      "loss: 13.720988166332244, data used: 100000 / 127560,  acc: 0.7983334064483643\n",
      "loss: 13.780880656838416, data used: 100000 / 127560,  acc: 0.8073333501815796\n",
      "loss: 13.659937244653701, data used: 100000 / 127560,  acc: 0.7950000762939453\n",
      "loss: 13.688782860338687, data used: 100000 / 127560,  acc: 0.7946667075157166\n",
      "loss: 13.781852334737778, data used: 100000 / 127560,  acc: 0.8016667366027832\n",
      "loss: 13.737705433368683, data used: 100000 / 127560,  acc: 0.8006667494773865\n",
      "loss: 13.66842544823885, data used: 100000 / 127560,  acc: 0.802333414554596\n",
      "loss: 13.732103106379508, data used: 100000 / 127560,  acc: 0.7963334321975708\n",
      "loss: 13.727726292610168, data used: 100000 / 127560,  acc: 0.7900000810623169\n",
      "loss: 13.701587820053101, data used: 100000 / 127560,  acc: 0.7953333854675293\n",
      "loss: 13.672215352952481, data used: 100000 / 127560,  acc: 0.7739999890327454\n",
      "loss: 13.738490253686905, data used: 100000 / 127560,  acc: 0.8070000410079956\n",
      "loss: 13.74048847258091, data used: 100000 / 127560,  acc: 0.8033334612846375\n",
      "loss: 13.737817391753197, data used: 100000 / 127560,  acc: 0.8036667704582214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-ab48ab6b4c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muntil_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.85\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-8a39edda8f0e>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(hm_epochs, learning_r, until_acc)\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0;31m# TensorFlow assigns the variables in feed_dict_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0;31m# to the placeholder variables and then runs the optimizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                         \u001b[0;31m#session.run(optimizer, feed_dict=feed_dict_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimize(until_acc=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating submission results...\n",
      "INFO:tensorflow:Restoring parameters from models/my_model\n",
      "Output Created!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "make_sub(test,y_pred_soft_max,session,best_validation_accuracy = best_validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 71.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.43645697832107544,\n",
       " 0.35772436857223511,\n",
       " 0.15508240461349487,\n",
       " 0.70114797,\n",
       " 0.64638805,\n",
       " 0.077457785606384277,\n",
       " 0.46851778030395508,\n",
       " 0.26048839092254639,\n",
       " 0.65228772,\n",
       " 0.48057132959365845]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saver.restore(session,'models/my_model')\n",
    "feed_dict_real_test = {x: X_real_test, y_true:y_real_test,keep_prob:1}\n",
    "print_accuracy(feed=feed_dict_test)\n",
    "a = session.run(y_pred_soft_max,feed_dict_test)\n",
    "b = a[:10]\n",
    "c= b.argmax(1)\n",
    "np.amax(b,1)\n",
    "[n if c[m] == 0 else 1-n for m,n in zip(range(len(np.amax(b,1))),np.amax(b,1))]\n",
    "#np.round(b,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57351166,  0.44273773,  0.16851619, ...,  0.52625763,\n",
       "        0.26620436,  0.80171454], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(cross_entropy,feed_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_dict_test[y_true][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43645707,  0.56354302],\n",
       "       [ 0.35772431,  0.64227563],\n",
       "       [ 0.15508242,  0.8449176 ],\n",
       "       [ 0.70114797,  0.298852  ],\n",
       "       [ 0.64638805,  0.35361198],\n",
       "       [ 0.0774578 ,  0.92254221],\n",
       "       [ 0.46851778,  0.53148222],\n",
       "       [ 0.26048836,  0.73951161],\n",
       "       [ 0.65228772,  0.34771228],\n",
       "       [ 0.48057133,  0.51942867]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5206.0699999999997"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ginic(y_50,y_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
