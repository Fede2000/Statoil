{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.cross_validation import train_test_split\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_and_ing_data() missing 3 required positional arguments: 'train_x', 'train_y', and 'test_dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec37b70428dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData_ing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_ing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_ing_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#X_train = train.drop(['id','target'],axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#X_train.replace(-1,-99999)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#y_train = train['target']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_and_ing_data() missing 3 required positional arguments: 'train_x', 'train_y', and 'test_dat'"
     ]
    }
   ],
   "source": [
    "import Data_ing\n",
    "X_train,y_train,test_dat = Data_ing.load_and_ing_data()\n",
    "#X_train = train.drop(['id','target'],axis=1)\n",
    "#X_train.replace(-1,-99999)\n",
    "#y_train = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-82a83e613556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrain_down_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_down_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mX_real_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0my_real_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_real_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#X_train, X_test, y_train, y_test = train_test_split(X_50 , y_50, test_size=0.02)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\"\"\"\n",
    "target_0 = train_y==0\n",
    "target_1 = train_y==1\n",
    "train_down_x = np.concatenate((train_x[target_1],train_x[target_1],train_x[target_1],\n",
    "                               train_x[target_0][0:int(sum(target_1)*3)]))\n",
    "train_down_y = np.concatenate((train_y[target_1],train_y[target_1],train_y[target_1],\n",
    "                               train_y[target_0][0:int(sum(target_1)*3)]))\n",
    "train_down_y = train_down_y.reshape([-1,1])\n",
    "\"\"\"\n",
    "X_real_test, y_real_test = [X_train[:3000],y_train[:3000]]\n",
    "y_real_test = y_real_test.reshape([-1,1])\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_50 , y_50, test_size=0.02)\n",
    "#X_train, y_train = shuffle(train_down_x,train_down_y, random_state=0)\n",
    "sm = SMOTE(random_state = 42)\n",
    "X_train, y_train = sm.fit_sample(X_train[3000:],y_train[3000:])\n",
    "print('smote Done')\n",
    "X_train, y_train = shuffle(X_train,y_train, random_state=0)\n",
    "print(Counter(y_train))\n",
    "y_train = y_train.reshape([-1,1])\n",
    "\n",
    "N_FEATURES=X_train.shape[1]\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-39495fef9bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:logits.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:multi_class_labels.dtype=<dtype: 'float32'>.\n",
      "INFO:tensorflow:losses.dtype=<dtype: 'float32'>.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "input_size = X_train.shape[1]\n",
    "\n",
    "# ### Placeholder variables\n",
    "x = tf.placeholder(tf.float32, [None, N_FEATURES],name=\"X-input\")\n",
    "y_true = tf.placeholder(tf.float32, [None,num_classes],name=\"y-input\") ##[None, num_classes]\n",
    "train_st = tf.placeholder(tf.bool, name = 'train_status')\n",
    "# ### Helper-functions for creating new variables\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.01), name = 'weight')\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05, shape=[length]),name='bias')\n",
    "\n",
    "\n",
    "# ### Helper-function for creating a new Fully-Connected Layer\n",
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,  # Use Rectified Linear Unit (ReLU)?\n",
    "                 use_sig=False):\n",
    "    # Create new weights and biases.\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif use_sig:\n",
    "        layer = tf.nn.sigmoid(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "# ###  Layers\n",
    "#hyperparameters\n",
    "n_nodes_hl1 = 200 #input_size*20\n",
    "n_nodes_hl2 = int(n_nodes_hl1/2)\n",
    "n_nodes_hl3 = int(n_nodes_hl2/4)\n",
    "n_nodes_hl4 = int(n_nodes_hl2/4)\n",
    "\n",
    "beta = 0.001  # l2 regularization\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "layer_h1 = new_fc_layer(input=x,\n",
    "                         num_inputs=input_size,\n",
    "                         num_outputs=n_nodes_hl1,\n",
    "                         use_relu=True)\n",
    "batch_norm_1 = tf.layers.batch_normalization(layer_h1,training=train_st)\n",
    "drop_h1 = tf.nn.dropout(batch_norm_1, 0.5 + 0.5*keep_prob)\n",
    "\n",
    "layer_h2 = new_fc_layer(input=drop_h1,\n",
    "                         num_inputs=n_nodes_hl1,\n",
    "                         num_outputs=n_nodes_hl2,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "batch_norm_2 = tf.layers.batch_normalization(layer_h2,training=train_st)\n",
    "drop_h2 = tf.nn.dropout(batch_norm_2, 0.75 + 0.25*keep_prob)\n",
    "\n",
    "layer_h3 = new_fc_layer(input=layer_h2,\n",
    "                         num_inputs=n_nodes_hl2,\n",
    "                         num_outputs=n_nodes_hl3,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "batch_norm_3 = tf.layers.batch_normalization(layer_h3,training=train_st)\n",
    "drop_h3 = tf.nn.dropout(layer_h3, 0.85 + 0.15*keep_prob)\n",
    "\n",
    "layer_h4 = new_fc_layer(input=drop_h3,\n",
    "                         num_inputs=n_nodes_hl3,\n",
    "                         num_outputs=n_nodes_hl4,\n",
    "                         use_relu=True,\n",
    "                         use_sig = False)\n",
    "batch_norm_4 = tf.layers.batch_normalization(layer_h4,training=train_st)\n",
    "drop_h4 = tf.nn.dropout(layer_h4, 0.9 + 0.1*keep_prob)\n",
    "\n",
    "y_pred = new_fc_layer(input=drop_h4,\n",
    "                         num_inputs=n_nodes_hl3,\n",
    "                         num_outputs=num_classes,\n",
    "                         use_relu=False,\n",
    "                         use_sig = False)\n",
    "y_pred_soft_max = tf.nn.sigmoid(y_pred)\n",
    "#y_pred_cls = tf.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# ### Cost-function and Optimizer\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "#loss = tf.losses.mean_squared_error(y_true,y_pred_soft_max)\n",
    "loss = tf.losses.sigmoid_cross_entropy(y_true,y_pred) #,weights= y_true*10\n",
    "#loss = tf.square(y_pred_soft_max - y_true)/2\n",
    "\n",
    "var_s   = tf.trainable_variables() \n",
    "lossL2 = tf.add_n([ tf.nn.l2_loss(v) for v in var_s\n",
    "                    if 'bias' not in v.name ]) * beta\n",
    "cost = tf.reduce_mean(loss)\n",
    "#error = tf.square(y_pred - y_true)/2 \n",
    "#cost = tf.reduce_sum(error + error* y_true*5)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)  #  1e-4\n",
    "\n",
    "# ### Performance measures\n",
    "correct_prediction = tf.equal(tf.round(y_pred_soft_max), y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test-set: 3.8%\n",
      "0.0200686016963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "#feed_dict_test = {x: X_test, y_true: y_test, keep_prob: 1.0}  #.values.reshape([-1,1])\n",
    "feed_dict_real_test = {x: X_real_test, y_true:y_real_test, keep_prob: 1.0,train_st:False}\n",
    "####\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def gini(y_actual, y_pred):\n",
    "  return 2*roc_auc_score(y_actual, y_pred)-1\n",
    "####\n",
    "def print_accuracy( r_valu = False, feed = feed_dict_real_test):\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed)\n",
    "    #print(precision_score(y_real_test,pred))\n",
    "    if r_valu:\n",
    "        return acc\n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))\n",
    "\n",
    "print_accuracy()\n",
    "from eval import normalized_gini\n",
    "def print_gini():\n",
    "    y_p,y = session.run([y_pred_soft_max,y_true], feed_dict=feed_dict_real_test)\n",
    "    #print(y[:,1])\n",
    "    return gini(y,y_p)\n",
    "print(print_gini())\n",
    "    \n",
    "saver = tf.train.Saver()  #search tf.saver.tensorflow\n",
    "best_validation_accuracy = 0\n",
    "best_validation_gini = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def optimize( hm_epochs = 10, learning_r = 1e-4):\n",
    "    global best_validation_accuracy,best_validation_gini\n",
    "    global global_step\n",
    "    for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            len_train = X_train.shape[0]\n",
    "            while i < len_train:\n",
    "                start = i\n",
    "                end = i + batch_size\n",
    "\n",
    "                batch_x = np.array(X_train[start:end])\n",
    "                batch_y = np.array(y_train[start:end])    #.values.reshape([-1,1]))\n",
    "                \n",
    "                feed_dict_train = {x: batch_x,\n",
    "                                   y_true: batch_y,\n",
    "                                   keep_prob: 0.0,\n",
    "                                   learning_rate:learning_r,\n",
    "                                   train_st:True}\n",
    "                \n",
    "                _, c = session.run([optimizer,cost], feed_dict=feed_dict_train)\n",
    "                epoch_loss += c\n",
    "\n",
    "                i += batch_size\n",
    "\n",
    "                if i % 50000 == 0  or i == (len_train - 1):\n",
    "\n",
    "\n",
    "                    acc_ = print_accuracy(True)\n",
    "                    if acc_ > best_validation_accuracy:\n",
    "                        best_validation_accuracy = acc_\n",
    "                        saver.save(session, 'models/my_model_acc')\n",
    "                        imp_str_acc = '*'*4\n",
    "                    else:\n",
    "                        imp_str_acc = ''\n",
    "                    gini = print_gini()\n",
    "                    if gini > best_validation_gini:\n",
    "                        best_validation_gini = gini\n",
    "                        saver.save(session, 'models/my_model_gini')\n",
    "                        imp_str_gini = '*'*4\n",
    "                    else:\n",
    "                        imp_str_gini = ''\n",
    "\n",
    "                    print('epoch:{} loss: {:.4f},  acc: {:.4f}{} __gini: {:.4f}{}'.format(epoch,epoch_loss,acc_*100,imp_str_acc,gini,imp_str_gini))\n",
    "                    epoch_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "    print('Epoch {} completed out of {}'.format(epoch+1,hm_epochs))\n",
    "\n",
    "    print('Optimization ended '+ 10*'_' + 'best accuracy: {}'.format(best_validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#best_validation_accuracy = 0.5\n",
    "#best_validation_gini= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77703a945587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-ed9f897418c3>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(hm_epochs, learning_r)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mlen_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "optimize(50,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3.07041802e-07],\n",
       "       [  8.84188651e-08],\n",
       "       [  4.14646564e-07],\n",
       "       ..., \n",
       "       [  2.80290891e-07],\n",
       "       [  1.87435290e-07],\n",
       "       [  6.19631749e-07]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(y_pred_soft_max,feed_dict_real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/my_model_acc\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "#X_sub = test.drop(['id'],axis=1)\n",
    "#X_sub.replace(-1,-99999)\n",
    "#X_sub = prepare_data(test,['id'])\n",
    "X_sub = test_dat\n",
    "\n",
    "saver.restore(session,\"models/my_model_acc\")\n",
    "sub1 = session.run(y_pred_soft_max,feed_dict={x:X_sub[:int(892816/2)],keep_prob:1.0,train:False})\n",
    "sub2 = session.run(y_pred_soft_max,feed_dict={x:X_sub[int(892816/2):],keep_prob:1.0,train:False})\n",
    "sub_tot_A = np.append(sub1,sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/my_model_gini\n"
     ]
    }
   ],
   "source": [
    "saver.restore(session,\"models/my_model_gini\")\n",
    "sub1 = session.run(y_pred_soft_max,feed_dict={x:X_sub[:int(892816/2)],keep_prob:1.0,train:False})\n",
    "sub2 = session.run(y_pred_soft_max,feed_dict={x:X_sub[int(892816/2):],keep_prob:1.0,train:False})\n",
    "sub_tot_B = np.append(sub1,sub2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test = pd.read_csv(\"data/test.csv\")\n",
    "test_id = test.id.values\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = (sub_tot_A + sub_tot_B)/2\n",
    "\n",
    "#series = pd.Series(sub_tot)\n",
    "sub.to_csv('sub/submission_{}.csv'.format(int(best_validation_accuracy*100)) ,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56610572,  0.52855325,  0.58747709,  0.63089669,  0.53465188,\n",
       "        0.58455157,  0.56880814,  0.66447335,  0.55966204,  0.54320621], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tot_B[sub_tot_B>0.5][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.4014574 ,  0.3196542 ,  0.12273712,  0.42307267,  0.34194165,\n",
       "        0.43101513,  0.4848755 ,  0.31742668,  0.29546708,  0.32031181], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_tot_A[sub_tot_B>0.5][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
